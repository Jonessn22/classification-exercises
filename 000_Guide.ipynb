{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6072dda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# import splitting and imputing functions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# confusion matrix for model evaluation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# **************************************\n",
    "\n",
    "# import local files\n",
    "import env\n",
    "import acquire\n",
    "\n",
    "import os\n",
    "\n",
    "# turn off pink boxes\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca35b1d",
   "metadata": {},
   "source": [
    "# <span style=\"color: #c48f7f\"> I. Data Acquisition </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9033d0",
   "metadata": {},
   "source": [
    "## Step 1: Connect to Codeup Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4e065f",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  DB Connect Function  |</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "952ff795",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to connect to Codeup Database\n",
    "\n",
    "def get_connection(db, user = env.user, host = env.host, password = env.password):\n",
    "    '''\n",
    "    This function uses my info from the env file to create a connection url that \n",
    "    returns the user credentials needed to access the requested Codeup database.\n",
    "    It takes in a string name of a database an an argument.\n",
    "    '''\n",
    "    return f'mysql+pymysql://{user}:{password}@{host}/{db}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9628503",
   "metadata": {},
   "source": [
    "## Step 2b: Read data from Codeup Database into DataFrame\n",
    "<i>(feeds into function in 2a)</i>\n",
    "\n",
    "- df_name = pd.read_sql('sql_query', get_connection('db_name')) ------> SQL QUERY\n",
    "- df_name = pd.read_csv('link/file_name/file_path/csv_export_url') ------------------>\n",
    "    - Google Sheet \n",
    "        - sheet_url = 'link'\n",
    "        - csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "- df_name = pd.read_excel('excel_file')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5cbdfb",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  Feed Read Function  |</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2053ca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_database_name_data():\n",
    "    '''\n",
    "    This function reads in the [database_name] data from the Codeup Database into a\n",
    "    Pandas DataFrame.\n",
    "    '''\n",
    "#     sequel query\n",
    "    sql_query = 'Select * from table_name'\n",
    "    \n",
    "#     read in DataFrame from Codeup DB\n",
    "    df_name = pd.read_sql(sql_query, get_connection('database_name'))\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e99f9e",
   "metadata": {},
   "source": [
    "## Step 2a: Cache Data\n",
    "Writing DataFrame to .csv file using df_name.to_file(file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d7d325",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  Read DB into DF Function  |</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b1a96b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_database_name_data():\n",
    "    '''\n",
    "    This function reads in the [database_name] data from the Codeup Database, writes\n",
    "    data to a .csv file if a local file does not already exist, and returns a df.\n",
    "    '''\n",
    "    if os.path.isfile('df_name.csv'):\n",
    "#         If .csv exists, read in data from .csv file\n",
    "        df_name = pd.read_csv('df_name.csv', index_col = 0)\n",
    "    \n",
    "    else:\n",
    "#         Read fresh data from Database into a DataFrame (referencing function from above cell)\n",
    "        df_name = new_database_name_data()\n",
    "        \n",
    "#         ... and write DataFrame to .csv file\n",
    "        df_name.to_csv('df_name.csv')\n",
    "    \n",
    "    return df_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb0bc77",
   "metadata": {},
   "source": [
    "# <span style=\"color: #c48f7f\">II. Data Preparation </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb28e9e8",
   "metadata": {},
   "source": [
    "## Step 1: Summarize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fa983f",
   "metadata": {},
   "source": [
    "Acquire and General Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f631efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire the data using acquire.py file function\n",
    "#     df_name = acquire.get_database_name_data()\n",
    "\n",
    "# sample of the DataFrame\n",
    "#     df_name.head()\n",
    "\n",
    "# numbers of rows and columns\n",
    "#     df_name.shape\n",
    "\n",
    "# information about the DataFrame:\n",
    "#     -- column names\n",
    "#     -- datatypes\n",
    "#     -- missing values\n",
    "#     df_name.info()\n",
    "\n",
    "# summary statistics for numeric columns\n",
    "#     df_name.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed7f63b",
   "metadata": {},
   "source": [
    "For loop to visualize numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc02e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to visualize the distributions for the numeric columns\n",
    "#     df_name_num_cols = df_name.columns[[df_name[col].dtype == 'int64' for col in df_name.columns]]\n",
    "\n",
    "#     for col in df_name_num_cols:\n",
    "#         plt.hist(df[col])\n",
    "#         plt.title(col)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57939bd",
   "metadata": {},
   "source": [
    "For loop to get breakdowns for object columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e14348c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for loop to get the breakdowns of the object columns\n",
    "#     df_name_obj_cols = df_name.columns[[df_name[col].dtype == 'O' for col in df_name.columns]]\n",
    "\n",
    "#     for col in obj_cols:\n",
    "#         print(df_name[col].value_counts())\n",
    "#         print(df[col].value_counts(normalize = True, dropna = False))\n",
    "#         print('-----------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625a101e",
   "metadata": {},
   "source": [
    "To bin columns with continuous numeric variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "031acc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to bin continuous numeric values\n",
    "#     df_name.column_name.value_counts(bins = x, sort = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa435074",
   "metadata": {},
   "source": [
    "To find missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb5278ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to find missing values\n",
    "#     missing = df_name.isnull().sum()\n",
    "#     missing[missing > 0]\n",
    "\n",
    "# or\n",
    "#     df.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce92e1d",
   "metadata": {},
   "source": [
    "## Step 2: Clean the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b38d4ec",
   "metadata": {},
   "source": [
    "Drop duplicates and fill missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33b7e8a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "#     df_name = df_name.drop_duplicates\n",
    "\n",
    "# Verify shape of data\n",
    "#     df_name.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407144fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns with too many missing values\n",
    "#     df_name_cols_to_drop = ['col_1', 'col_2' ...]\n",
    "#     df_name = df_name.drop(columns = df_name_cols_to_drop)\n",
    "\n",
    "# Verify shape of data\n",
    "#     df_name.shape\n",
    "\n",
    "# Preview DataFrame and verify columns were dropped\n",
    "#     df_name.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "313435bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in missing values with most common value\n",
    "#     df_name['column_name'] = df_name.column_name.fillna(value = 'fill_value')\n",
    "\n",
    "# Validate that missing values have been filled (this line of code should return 0)\n",
    "#     df_name.column_name.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a18d9d",
   "metadata": {},
   "source": [
    "Create dummy variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bc134c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy DataFrame\n",
    "#     df_name_dummies = pd.get_dummies(df_name[['col_1', 'col_2' ...]], dummy_na = False,\n",
    "#                                                                         drop_first = [True])\n",
    "#     df_name_dummies.head()\n",
    "\n",
    "# Concatenate the dummy DataFrame with original DataFrame\n",
    "#     df_name = pd.concat([df_name, df_name_dummies], axis = 1)\n",
    "#     df_name.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda73d2",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  Clean Data Function  |</span></span>\n",
    "- drops duplicates\n",
    "- fills missing values\n",
    "- creates dummy vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59d11d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_data(df):\n",
    "#     '''\n",
    "#     This function cleans the data and does the following:\n",
    "#         - drops duplicate observations\n",
    "#         - drops columns with too many missing values ['col_1', 'col_2', ...]\n",
    "#         - fill missing values with most common, 'common_value'\n",
    "#         - creates dummy variables from col_1, col_2, ...\n",
    "#     '''\n",
    "#     df = df.drop_duplicates()\n",
    "#     df = df.drop(columns = ['col_drop_1', 'col_drop_2' ...])\n",
    "    \n",
    "#     df['fill_col'] = df.fill_col.fillna(value = 'fill_value')\n",
    "    \n",
    "#     dummy_df = pd.get_dummies(df[['dum_col_1', 'dum_col_2' ...]], drop_first = True)\n",
    "#     df = pd.concat([df, dummy_df], axis = 1)\n",
    "    \n",
    "#     return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd8cfb3",
   "metadata": {},
   "source": [
    "## Step 3: Split Data\n",
    "#### | Train | *** | Validate | *** | Test |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33ac3035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20% test, 80% train_validate\n",
    "#     of the 80% train_validate: 30% validate, 70% train\n",
    "#     .24% validate, .56 train\n",
    "\n",
    "# train, test = train_test_split(df, test_size = .2, \n",
    "#                                random_state = 123,\n",
    "#                               stratify = df.target)\n",
    "\n",
    "# train, validate = train_test_split(train, test_size = .3,\n",
    "#                                    random_state = 123,\n",
    "#                                    stratify = train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67d91df",
   "metadata": {},
   "source": [
    "Validate the split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b81239d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'train ------> {train.shape}')\n",
    "# print(f'validate ------> {validate.shape}')\n",
    "# print(f'test ------> {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7242c209",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  Split Data Function  |</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bad777aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, target, seed = 123):\n",
    "    '''\n",
    "    This function takes in a DataFrame, a target variable (for stratification purposes), and an integer for\n",
    "    setting a seed and splits the data into train, validate, and test DataFrames;\n",
    "    and stratifies on the target variable\n",
    "    '''\n",
    "    train_validate, test = train_test_split(df, test_size = .2,\n",
    "                                           random_state = seed,\n",
    "                                           stratify = df[target])\n",
    "    \n",
    "    train, validate = train_test_split(train_validate, test_size = .3,\n",
    "                                      random_state = seed,\n",
    "                                      stratify = train_validate[target])\n",
    "    \n",
    "    return train, validate, test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e4608",
   "metadata": {},
   "source": [
    "Test out the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a52b65ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validate, test = split_data(df, target = 'target_variable')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9201497b",
   "metadata": {},
   "source": [
    "Validate my split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5c2858c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f'train -------> {train.shape}')\n",
    "# print(f'validate ----> {validate.shape}')\n",
    "# print(f'test --------> {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f50e75",
   "metadata": {},
   "source": [
    "## Step 4: Imputing Missing Values\n",
    "1. Create the imputer object, selecting the strategy used to impute\n",
    "    - Mean\n",
    "    - Median\n",
    "    - Mode (strategy = 'most_frequent')<br><br>\n",
    "2. Fit to train \n",
    "    - Compute the mean, median, or most_frequent (mode) for each of the columns that will be imputed.\n",
    "    - Store that value in the imputer object<br><br>\n",
    "2. Transform train: fill missing values in the train dataset with that value identified.<br><br>\n",
    "2. Transform validate and test: fill missing values with that value identified].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c47566f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only look at the train dataset after data split\n",
    "#     train.info()\n",
    "\n",
    "# 1. Create the SimpleImputer object (imputer instructions)---> will be stored in a variable called imputer\n",
    "#     imputer = SimpleImputer(missing_values = None, strategy = 'most_frequent')\n",
    "\n",
    "# 2. Fit the imputer columns in the training df so the imputer determines the value depending on the strategy \n",
    "# called\n",
    "#     imputer = imputer.fit(train[['col_name']])\n",
    "\n",
    "\n",
    "# 3. Next we will call transform on all three of our split data sets\n",
    "#     train[['col_name']] = imputer.transform(train[['col_name']])\n",
    "\n",
    "# 4. And finally calling transform on our validate and test data sets\n",
    "#     validate[['col_name']] = imputer.transform(validate[['col_name']])\n",
    "#     test[['col_name']] = imputer.transform(test[['col_name']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a4a8805c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate there are no longer any null values in imputer column(s)\n",
    "#     train.col_name.value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9f1280",
   "metadata": {},
   "source": [
    "### <span style=\"background-color: #c48f7f\"><span style=\"color: #ffffff\">|  Imputer Function  |</span></span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa9d9af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def impute_mode(train, validate, test):\n",
    "#     '''\n",
    "#     This function takes in the train, test, and validate DataFrames and imputes the mode for the selected\n",
    "#     column to impute, returning imputed train, test, and validated DataFrames\n",
    "#     '''\n",
    "#     imputer = SimpleImputer(missing_values = None, strategy = 'most_frequent')\n",
    "#     train[['col_name']] = imputer.fit_transform(train[['col_name']])\n",
    "#     validate[['col_name']] = imputer.transform(validate[['col_name']])\n",
    "#     test[['col_name']] = imputer.transform(test[['col_name']])\n",
    "    \n",
    "#     return train, validate, test\n",
    "\n",
    "# *********************************\n",
    "\n",
    "# Validate the function worked properly\n",
    "#     train, validate, test = prep_titanic_data(df)\n",
    "#     train.info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c465f7",
   "metadata": {},
   "source": [
    "# <span style=\"color: #c48f7f\">III. Tidy Data </style>\n",
    "- Data should be tabular (made up of rows and columns)\n",
    "- There is only value per cell\n",
    "- Each variable should have its own column\n",
    "- Each observation should have its own row\n",
    "\n",
    "\n",
    "<b>Melt</b> use when one variable is spread across multiple columns\n",
    "- Wide ----> Long\n",
    "\n",
    "<b>Pivot</b> use when one column contains multiple variables\n",
    "- Long ----> Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74f95baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# melt\n",
    "\n",
    "# |df_name.melt(id_vars = ['index_column'], var_name = 'new_var_column_name', value_name = 'value_column_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fc37aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pivot\n",
    "\n",
    "# |df_name.pivot(index = 'index_column', columns = 'column_to_pivot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95184fc",
   "metadata": {},
   "source": [
    "# <span style=\"color: #c48f7f\">IV. Data Exploration </style>\n",
    "<span style=\"color: #c48f7f\"><b>Exploratory Data Analysis</style> (EDA)</b> where we develop nearly all of the insights and takeaways and learn the <i>story</i> of our data.\n",
    "### Only explore train data set!\n",
    "##### <span style=\"color: #c48f7f\">process of performing initial investigations on data so as to:</style>\n",
    "    - discover patterns,\n",
    "    - spot anomolies,\n",
    "    - test hypothesis, and\n",
    "    - check assumptions\n",
    "##### <span style=\"color: #c48f7f\">with the help of:</style>\n",
    "    - summary statistics and\n",
    "    - graphical representations    \n",
    "##### <span style=\"color: #c48f7f\">can lead to:</style>\n",
    "    - feature engineering,\n",
    "    - feature elimination to reduce noise, and\n",
    "    - domain based outlier handling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cdaac1",
   "metadata": {},
   "source": [
    "## Step 1: Document initial hypotheses \n",
    "How the indepedent variables (predictors, features, attributes) interact with the target (y-value or dependent variable) using natural language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264e3f13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "043ee478",
   "metadata": {},
   "source": [
    "## Step 2: Use visualization techniques to identify drivers\n",
    "### i. Univariate Stats\n",
    "- Looking for outliers\n",
    "- Testing for normalcy\n",
    "- Looking at the scale of each variable<br><br>\n",
    "###### Using:\n",
    "- value_counts( ) [categorical variables] and\n",
    "- histograms [numerical variables]\n",
    "- frequencies\n",
    "\n",
    "### ii. Bivariate Stats\n",
    "Plot the interactions of each variable with the target and document takeaways.\n",
    "- Numeric -----> Numeric\n",
    "    - Scatterplot\n",
    "    - Lineplot\n",
    "    \n",
    "- Numeric -----> Categorical\n",
    "    - see https://seaborn.pydata.org/tutorial/categorical.html\n",
    "    \n",
    "<b><i>Use hypothesis testing where appropriate.</b></i>\n",
    "\n",
    "### iii. Multivariate Stats\n",
    "Ask additional questions of the data, such as how subgroups compare to each other and to the overall population using visualizations and/or hypothesis testing.\n",
    "- sns.pairplot (with hue and/or col)\n",
    "- see https://seaborn.pydata.org/tutorial/axis_grids.html\n",
    "\n",
    "<b><i>Use hypothesis testing where appropriate.</b></i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a8262f",
   "metadata": {},
   "source": [
    "## Step 3: Hypothesis Testing\n",
    "When a visualization isn't immediately clear or you need/want additional confirmation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1d7860",
   "metadata": {},
   "source": [
    "## Which Hypothesis Test?\n",
    "### Pearson's R\n",
    "<b>corr, p = stats.pearsonsr( train_df.column, train_df.column )</b><br><br>\n",
    "https://ds.codeup.com/stats/more-statistical-testing-examples/#pearson-r\n",
    "\n",
    "- Numeric ------> Numeric\n",
    "- Linear relationships\n",
    "\n",
    "### Spearman's R\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html\n",
    "- Numeric -------> Numeric\n",
    "- Non-linear relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ad428",
   "metadata": {},
   "source": [
    "### T-Test\n",
    "<b>t, p = stats.ttest_1samp( train_df_sample, $\\mu$ )<br>\n",
    "t, p = stats.ttest_ind( train_df_sample1, train_df_sample2, equal_var = True/False )</b><br><br>\n",
    "https://ds.codeup.com/stats/compare-means/\n",
    "- Numeric --------> Categorical\n",
    "- Comparing the means of two populations\n",
    "- Comparing the mean of a subgroup with the mean of the total population\n",
    "- When samples are normal(ish) distributed but have different variances <i>(determined using .var( ))</i>\n",
    "\n",
    "### ANOVA\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.f_oneway.html\n",
    "- Numeric --------> Categorical\n",
    "- Comparing the means of more than two groups\n",
    "\n",
    "### Mann-Whitney u-test\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "- Numeric --------> Categorical\n",
    "- Data does not match the assumptions of a t-test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3e268e",
   "metadata": {},
   "source": [
    "### $chi^2$ Test\n",
    "<b>observed = pd.crosstab( a, b )<br>\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)</b><br><br>\n",
    "https://ds.codeup.com/stats/compare-group-membership/\n",
    "- Categorical ----> Categorical "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ab83a54",
   "metadata": {},
   "source": [
    "\n",
    "## Univariate Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f0b094f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# |.info() -----> determine which variables are quantitative (numeric) vs qualitative (non-numeric) by datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a3030f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantitative (numeric) variables\n",
    "#     .describe() ----> get descriptive statistics for quantitative variables\n",
    "#     start by plotting the target variable (document takeaways)\n",
    "#     use list comprehension to create a for loop that saves each numeric column into a variable ---> num_cols\n",
    "#         |train_df_num_cols = train_df.columns[[train_df[col].dtype == 'float'for col in train_df.columns]]\n",
    "#     use a for loop to plot (hist&boxplot) each numeric column from num_cols list created in last step\n",
    "#         |for col in train_df_num_cols:\n",
    "#             |plt.hist(train_df[col])\n",
    "#             |plt.title(col)\n",
    "#             |plt.show()\n",
    "#             |plt.boxplot(train[col])\n",
    "#             |plt.title(col)\n",
    "#             |plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c081914f",
   "metadata": {},
   "source": [
    "\n",
    "## Bivariate Exploration\n",
    "##### Analyzing each feature with respect to the target variable\n",
    "<b>Document: $H$(o), $H$(a), and $\\alpha$</b>\n",
    "- ask and document questions and document takeaways\n",
    "- numeric ------> numeric\n",
    "    - scatterplot\n",
    "- numeric ------> categorical\n",
    "    - catplot\n",
    "    - barplot\n",
    "    - boxplot\n",
    "- think about other feature combinations we could visualize\n",
    "- could we bin any features?\n",
    "- should we create any new features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e6a7333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# qualitative (non-numeric) variables\n",
    "#     |plt.hist(train.df[col])\n",
    "#     |train_df[col].value_counts()\n",
    "#     compute the descriptive statistics within each categorical value (from value_counts) (for each type of ...\n",
    "#     ...flower) ---> can create df's for each and then concatenate\n",
    "#         use a barplot to plot the different categorical values against one another\n",
    "#             |plt.title('Title')\n",
    "#             |sns.barplot(x = 'categorical_column', y = 'numerical_column', data = train_df)\n",
    "#         plot a mean line on the barplot\n",
    "#             |target_rate = train_df.target.mean()\n",
    "#             |plt.axhline(target_rate, label = 'Average Target')\n",
    "#         use a hypothesis test to compare the means across categorical values, if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bda266d",
   "metadata": {},
   "source": [
    "\n",
    "## Multivariate Exploration\n",
    "##### Adding an additional dimension to our data, such as the target variable as color or, separating variable values by columns\n",
    "- Here we are asking more specific and targeted questions\n",
    "    - How subgroups compare to one another\n",
    "    - How subgroups compare to the target populuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f448f4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting\n",
    "#     add hue and/or cols to existing plots\n",
    "#     pairplot\n",
    "#         |sns.pairplot(train_df, hue = 'target', corner = True)\n",
    "#         |plt.show()\n",
    "#     heatmap\n",
    "#         |sns.heatmap(train_df.corr(), cmap = 'color_combo', center = x, annot = True)\n",
    "#         |plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97dcdea3",
   "metadata": {},
   "source": [
    "# <span style=\"color: #c48f7f\">V. Evaluation </style>\n",
    "How we evaluate our classification model's performance\n",
    "##### The methods used in this section, along with the classification_report function are to evaluate the model performance:\n",
    "- Train ---------> see in-sample performance\n",
    "- Validate ------> see out-of-sample performance and allow us to tune parameters\n",
    "- Test "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874dbaaa",
   "metadata": {},
   "source": [
    "## Confusion Matrix\n",
    "A cross-tabulation of our model's predictions against the actual values\n",
    "- <b>Positive</b>\n",
    "    - tp:\n",
    "    - fp: [over-confident]\n",
    "- <b>Negative</b>\n",
    "    - tn:\n",
    "    - fn: [under-confident]\n",
    "- <b>Consequences</b>\n",
    "    - fp\n",
    "    - fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e1eef",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "- <b>Accuracy</b><br>\n",
    "(TP + TN) / total\n",
    "    - Will be the same regardless of which is positive value\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c846063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
